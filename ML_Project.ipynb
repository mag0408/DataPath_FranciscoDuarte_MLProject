{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmb9DtwrcBY5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pickle\n",
        "\n",
        "\n",
        "# Step 1: Load the dataset using pandas\n",
        "def load_dataset(filename):\n",
        "    data = pd.read_csv(filename)\n",
        "    return data\n",
        "\n",
        "# Step 2: Function to count and describe NaN values in the dataset\n",
        "def describe_nan_values(data):\n",
        "    nan_counts = data.isnull().sum()\n",
        "    nan_description = data.describe(include='all')\n",
        "    return nan_counts, nan_description\n",
        "\n",
        "# Step 3: Function to visualize attributes with histograms\n",
        "def visualize_histograms(data):\n",
        "    numeric_columns = data.select_dtypes(include=np.number)\n",
        "    for col in numeric_columns.columns:\n",
        "        sns.histplot(data[col], kde=True)\n",
        "        plt.title(f'Histogram of {col}')\n",
        "        plt.show()\n",
        "\n",
        "# Step 4: Split the dataset into train, cross-validation, and test sets\n",
        "def split_dataset(data):\n",
        "    X = data.iloc[:, :-1]  # Features (all columns except the last one)\n",
        "    y = data['Class']      # Target variable\n",
        "\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    X_cv, X_test, y_cv, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    return X_train, y_train, X_cv, y_cv, X_test, y_test\n",
        "\n",
        "# Step 5: Normalize/Scale the input data using mean and variance\n",
        "def normalize_data(X_train, X_cv, X_test):\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_cv_scaled = scaler.transform(X_cv)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    # Save the scaler to a file using pickle\n",
        "    with open(\"scaler.pkl\", 'wb') as scaler_file:\n",
        "      pickle.dump(scaler, scaler_file)\n",
        "    return X_train_scaled, X_cv_scaled, X_test_scaled\n",
        "\n",
        "# Step 6: Train the model with different learning rates and select the best hyperparameters\n",
        "def train_model(X_train, y_train, learning_rates):\n",
        "    best_lr = None\n",
        "    best_score = 0\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
        "        scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "        avg_score = np.mean(scores)\n",
        "        if avg_score > best_score:\n",
        "            best_score = avg_score\n",
        "            best_lr = lr\n",
        "\n",
        "    return best_lr\n",
        "\n",
        "# Step 7: Train the final model with the best hyperparameters\n",
        "def train_final_model(X_train, y_train, best_lr):\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "# Step 8: Evaluate the model using cross-validation data\n",
        "def evaluate_model(model, X_cv, y_cv):\n",
        "    cv_score = model.score(X_cv, y_cv)\n",
        "    return cv_score\n",
        "\n",
        "# Step 9: Test the model with the test dataset\n",
        "def test_model(model, X_test, y_test):\n",
        "    test_score = model.score(X_test, y_test)\n",
        "    return test_score\n",
        "\n",
        "\n",
        "# Step 10: Save the trained model using pickle\n",
        "def save_model(model, filename):\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(model, file)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Step 1: Load the dataset\n",
        "    dataset = load_dataset(\"pulsar.csv\")\n",
        "\n",
        "    # Step 2: Describe NaN values in the dataset\n",
        "    nan_counts, nan_description = describe_nan_values(dataset)\n",
        "    print(\"\\nNaN Counts:\")\n",
        "    print(nan_counts)\n",
        "    print(\"\\nNaN Description:\")\n",
        "    print(nan_description)\n",
        "\n",
        "    # Step 3: Visualize attributes with histograms\n",
        "    visualize_histograms(dataset)\n",
        "\n",
        "    # Step 4: Split the dataset\n",
        "    X_train, y_train, X_cv, y_cv, X_test, y_test = split_dataset(dataset)\n",
        "\n",
        "    # Step 5: Normalize the data\n",
        "    X_train_scaled, X_cv_scaled, X_test_scaled = normalize_data(X_train, X_cv, X_test)\n",
        "\n",
        "    # Step 6: Train the model with different learning rates\n",
        "    learning_rates = [0.001, 0.01, 0.1, 1]\n",
        "    best_lr = train_model(X_train_scaled, y_train, learning_rates)\n",
        "\n",
        "    # Step 7: Train the final model with the best hyperparameters\n",
        "    final_model = train_final_model(X_train_scaled, y_train, best_lr)\n",
        "\n",
        "    # Step 8: Evaluate the model using cross-validation data\n",
        "    cv_score = evaluate_model(final_model, X_cv_scaled, y_cv)\n",
        "    print(f\"Cross-validation score: {cv_score}\")\n",
        "\n",
        "    # Step 9: Test the model with the test dataset\n",
        "    test_score = test_model(final_model, X_test_scaled, y_test)\n",
        "    print(f\"Test score: {test_score}\")\n",
        "\n",
        "    # Step 10: Save the trained model\n",
        "    save_model(final_model, \"pulsar_model.pkl\")\n",
        "\n",
        "\n"
      ]
    }
  ]
}